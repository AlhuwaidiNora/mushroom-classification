{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative; text-align: right;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98614301-dcf01780-22d6-11eb-9c8f-65ebfceac6f6.png\" style=\"width: 130px; display: inline-block;\"></img>\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98864025-08deda80-2448-11eb-9600-22aa17884cdf.png\" style=\"height: 100%; max-height: inherit; position: absolute; top: 20%; left: 0px;\"></img>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"font-weight: bold;\">\n",
    "    Kristin Day\n",
    "</h2>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; margin-top: 0.8em\">\n",
    "    Data Scientist\n",
    "</h3>\n",
    "<hr>\n",
    "<br><br>\n",
    "\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    yokristinday@gmail.com\n",
    "</p>\n",
    "<p style=\"font-size: 80%; text-align: right; margin: 10px 0px;\">\n",
    "    linkedin.com/in/kristin-day-300306a9\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98728503-5ab82f80-2378-11eb-9c79-adeb308fc647.png\"></img>\n",
    "\n",
    "<h3 style=\"color: white; position: absolute; top:30%; left:10%;\">\n",
    "    Decision Trees\n",
    "</h3>\n",
    "<h1 style=\"color: white; position: absolute; top:35%; left:10%;\">\n",
    "    Mushroom Classification\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color: #ef7d22; font-weight: normal; position: absolute; top:43%; left:10%;\">\n",
    "    Kristin Day\n",
    "</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; background-color: #ef7d22; text-align: center\">\n",
    "<br><br>\n",
    "\n",
    "<h1 style=\"color: white; font-weight: bold;\">\n",
    "    Classify the mushrooms into edible or non-edible classes using a decision tree algorithm.\n",
    "    Be as accurate as possible - it's a matter of life or death!\n",
    "</h1>\n",
    "\n",
    "\n",
    "<br><br> \n",
    "</div>\n",
    "\n",
    "![mushrooms](https://user-images.githubusercontent.com/7065401/112419584-6a725800-8d0a-11eb-8c9a-faaf2ba2a08d.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Pull in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in your data\n",
    "filepath = 'data/mushrooms.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.5: Convert categorical columns to numeric using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas .info() method to see what columns need to be converted\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All of the columns are type=object so they all need to be encoded\n",
    "### But, we actually fit our label encoder on our train data and then transform our test data\n",
    "### Need to split the data before we can continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"class\" is the target column\n",
    "# Split data into test and train - remember to stratify on y\n",
    "X = data.drop('class', axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Split using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can label encode everything\n",
    "from collections import defaultdict\n",
    "d = defaultdict(LabelEncoder)\n",
    "# Encoding the variable\n",
    "X_train_encoded = X_train.apply(lambda x: d[x.name].fit_transform(x))\n",
    "# Using the dictionary to label future data\n",
    "X_test_encoded = X_test.apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "# Encode y data\n",
    "le_y = LabelEncoder()\n",
    "y_train_encoded = le_y.fit_transform(y_train)\n",
    "y_test_encoded = le_y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Instantiate your machine learning class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you set hyperparameters for model tuning\n",
    "# Set up a param distribution like we did in the lessons\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Instantiate a decision tree classifier using criterion='gini' and set a random state\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Instantiate the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=10, cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the randomized search class on the train data\n",
    "random_search.fit(X_train_encoded, y_train_encoded)\n",
    "\n",
    "# Save the best hyperparameters into a dictionary\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best params to instantiate a new decision tree classifier\n",
    "best_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "best_clf.fit(X_train_encoded, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Fit the model on your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tuned classifier to the train data.\n",
    "best_clf.fit(X_train_encoded, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Make predictions on the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the train data\n",
    "train_predictions = best_clf.predict(X_train_encoded)\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = best_clf.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the baseline accuracy. For classification, it's the percentage of the majority class\n",
    "baseline_accuracy = y_train_encoded.value_counts().max() / len(y_train_encoded)\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n",
    "\n",
    "# Get class value counts for y_train\n",
    "class_counts = y_train_encoded.value_counts()\n",
    "print(f\"Class Counts in y_train: {class_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train_encoded, train_predictions)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing accuracy\n",
    "test_accuracy = accuracy_score(y_test_encoded, test_predictions)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at some of the actual test values\n",
    "test_values = pd.DataFrame({'Actual': y_test_encoded, 'Predicted': test_predictions})\n",
    "print(test_values.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to some of the predicted test values (this is a spot check)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_encoded, test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using some simple pre-pruning with this classification leads to perfect accuracy on this dataset.\n",
    "# Don't get used to it! This rarely happens in real life!\n",
    "\n",
    "# Additional Tip:\n",
    "Sometimes it's good to have a separate holdout set for validating results.  If you want to be very sure about your production accuracy, keep a separate chunk of data for validation.  Once you've made all of the adjustments to your model, run the model on the validation set to see your expected production accuracy.  After you run the validation set through, if you make changes to the model to improve performance, your validation set will become just another test set.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Get some information from your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at our decision tree - plot it using plot_tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(best_clf, feature_names=X.columns, class_names=le_y.classes_, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use heapq to print the features in order of importance\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the features in order of importance\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train_encoded.shape[1]):\n",
    "    print(f\"{f + 1}. {X_train_encoded.columns[indices[f]]} ({importances[indices[f]]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;\">\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/98729912-57be3e80-237a-11eb-80e4-233ac344b391.png\"></img>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
